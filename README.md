# ADDNet — Attention-based Deepfake Detection Networks

Implementation of **ADDNet** from the paper  
[*Attention-based Deepfake Detection*](https://arxiv.org/pdf/2101.01456) (arXiv 2101.01456),  
extended with a novel **learnable bandpass layer** for frequency-domain preprocessing.

---

## Overview

Deepfake videos are typically generated by an attention-based face fusion operation:

$$O = t \odot (E - A) + g \odot A$$

where $t$ is the source (true) face, $g$ is the generated face, $A \in [0,1]^d$ is the attention mask, and $\odot$ is the element-wise product.

The core idea of ADDNet is to exploit this exact attention mechanism during detection: facial attention masks are injected at multiple intermediate layers of the backbone, guiding the network to focus on the regions most likely altered by deepfake synthesis (eyes, nose, mouth).

This repository contains two model variants:

| Model | Description |
|---|---|
| `ADDNet2D_Xception` | Direct implementation of ADDNet-2D from the paper |
| `ADDNet2D_Xception_Bandpass` | Extension with a learnable bandpass preprocessing layer |

---

## Architecture

### ADD Block

The `ADDBlock` is the fundamental attention injection unit. At each selected layer of the backbone it:

1. Rescales the input attention mask to match the current feature map spatial resolution via `AdaptiveAvgPool2d`.
2. Applies element-wise multiplication between the scaled mask and the feature map.

This adjusts feature activations at multiple abstraction levels, emphasising the face and organ regions throughout the network.

### ADDNet-2D (Xception backbone)

```
Input image [B, 3, 299, 299]
Attention mask [B, 1, 299, 299]
        │
   ┌────▼────┐
   │  Stem   │  (conv1 → bn → act → conv2 → bn → act)
   └────┬────┘
        │
  Entry Flow
   block1 → ADD Block
   block2 → ADD Block
   block3 → ADD Block
        │
 Middle Flow (blocks 4–11) → ADD Block
        │
  Exit Flow (block12) → ADD Block
        │
  conv3 → conv4 (final feature extraction)
        │
  GlobalAvgPool → Dropout(0.5) → FC(2048 → 2)
        │
  Logits [B, 2]
```

### ADDNet-2D + Bandpass (proposed extension)

Adds a **learnable `BandpassLayer`** as a preprocessing step before the Xception backbone. The layer computes the difference of two Gaussian blurs with trainable standard deviations $\sigma_\text{low}$ and $\sigma_\text{high} = \sigma_\text{low} + \Delta$:

$$\text{BandpassLayer}(x) = G_{\sigma_\text{low}}(x) - G_{\sigma_\text{high}}(x)$$

Both $\sigma_\text{low}$ and $\Delta$ are learnable parameters, allowing the model to discover the frequency band most discriminative for deepfake detection. The full pipeline becomes:

```
Input image → BandpassLayer → Xception + ADD Blocks → Classification
```

### Attention Mask Generation

Implemented in `attention_mask.py` and `dataset.py` following the paper's Figure 6:

1. **Face alignment** — eyes are detected via dlib 68-point landmarks; the image is rotated to level the eye line.
2. **Face mask** — convex hull around all 68 landmarks, filled and Gaussian-blurred.
3. **Organ mask** — convex hull around eyes (36–47), nose (27–35) and mouth (48–67), filled and Gaussian-blurred.
4. **Final mask** — face mask + organ mask, normalised to $[0, 1]$.

---

## Repository Structure

```
addnet/
├── addnet.py              # ADDNet2D_Xception model
├── addnet_bandpass.py     # ADDNet2D_Xception_Bandpass model (+ BandpassLayer)
├── attention_mask.py      # AttentionMask generation module (standalone)
├── dataset.py             # DeepfakeDataset — loads images and generates masks on-the-fly
├── filter.py              # Frequency-domain bandpass filter utility (NumPy / OpenCV)
├── train.py               # Training script for ADDNet2D_Xception
├── train_bandpass.py      # Training script for ADDNet2D_Xception_Bandpass
└── data/
    ├── train/
    │   ├── real/
    │   └── fake/
    └── test/
        ├── real/
        └── fake/
```

---

## Requirements

```bash
pip install torch torchvision timm dlib opencv-python numpy scikit-learn tqdm comet_ml
```

> **dlib landmark model:** download `shape_predictor_68_face_landmarks.dat` from the [dlib model zoo](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2) and place it one level above the project root (or update the path in `dataset.py`).

---

## Usage

### Dataset layout

```
<data_dir>/
    train/
        real/   *.jpg / *.png
        fake/   *.jpg / *.png
    test/
        real/
        fake/
```

Images should be cropped face images (299 × 299 recommended for Xception).

### Training ADDNet-2D

```bash
python train.py \
    --data_dir /path/to/dataset \
    --batch_size 16 \
    --epochs 100 \
    --lr 1e-4 \
    --device cuda:0 \
    --save_dir ../saved_models/addnet \
    --project_name addnet-deepfake
```

### Training ADDNet-2D + Bandpass

```bash
python train_bandpass.py \
    --data_dir /path/to/dataset \
    --batch_size 16 \
    --epochs 100 \
    --lr 1e-4 \
    --kernel_size 31 \
    --device cuda:0 \
    --save_dir ../saved_models/addnet_bandpass \
    --project_name addnet-deepfake-bandpass
```

Pass `--offline` to log experiments locally without a Comet ML API key.

### Quick model test

```bash
# ADDNet-2D
python addnet.py

# ADDNet-2D + Bandpass
python addnet_bandpass.py
```

---

## Key Arguments

| Argument | Default | Description |
|---|---|---|
| `--data_dir` | — | Root directory of the dataset |
| `--batch_size` | `16` | Training batch size |
| `--epochs` | `100` | Number of training epochs |
| `--lr` | `1e-4` | Learning rate (Adam) |
| `--weight_decay` | `1e-5` | L2 regularisation |
| `--image_size` | `299` | Input resolution |
| `--device` | `cuda:0` | PyTorch device |
| `--pretrained` | `True` | ImageNet-pretrained Xception |
| `--resume` | `None` | Path to checkpoint to resume from |
| `--kernel_size` | `31` | Bandpass kernel size *(bandpass only)* |

---

## Experiment Tracking

Both training scripts integrate with [Comet ML](https://www.comet.com). Logged metrics include:

- Train / validation loss and accuracy per epoch
- Confusion matrix and full classification report
- Bandpass layer parameters ($\sigma_\text{low}$, $\Delta$, $\sigma_\text{high}$) per epoch *(bandpass variant)*
- Filtered image visualisations *(bandpass variant)*

---

## Reference

```bibtex
@article{zi2020wilddeepfake,
  title   = {WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection},
  author  = {Zi, Bojia and Chang, Minghao and Chen, Jingjing and Ma, Xingjun and Jiang, Yu-Gang},
  journal = {arXiv preprint arXiv:2101.01456},
  year    = {2021}
}
```
